{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd830f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import requests\n",
    "import zipfile\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from datetime import timezone\n",
    "import shutil\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "\n",
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "curdir = os.path.dirname(__file__).replace(\"\\\\\",\"/\")\n",
    "time_interval = 1  # Total number of hours we want to look back into\n",
    "mongodb_account = 'quantumgaihold'\n",
    "mongodb_password = 'ZBYamTQ4EMDVgKl5'\n",
    "\n",
    "\n",
    "# Establish connection to MongoDB\n",
    "\n",
    "uri = \"mongodb+srv://\" + mongodb_account + \":\" + mongodb_password + \"@cluster0.etjr4e4.mongodb.net/?retryWrites=true&w=majority\"\n",
    "client = MongoClient(uri, server_api=ServerApi('1'))             \n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "db = client.lithiumdash # MongoDB database name\n",
    "collection_lithium_15 = db.lithium_data_15\n",
    "collection_lithium_map_15=db.lithium_map_15\n",
    "\n",
    "### First clear all existing data to ensure that all data are up-to-date\n",
    "collection_lithium_15.delete_many({})\n",
    "collection_lithium_map_15.delete_many({})\n",
    "print(\"Removed outdated data from MongoDB\\n\")\n",
    "\n",
    "# get current date and time\n",
    "now = datetime.now(timezone.utc)\n",
    "t = now.strftime(\"%D,%H,%M,%S\")\n",
    "t = t.split(\",\")\n",
    "t[0] = t[0].split(\"/\")\n",
    "\n",
    "# round to nearest 15 minutes\n",
    "# decide first date of range based on closest time interval\n",
    "if ( int(t[2]) >= 0 ) & ( int(t[2]) < 15 ):\n",
    "    m = '00'\n",
    "elif ( int(t[2]) >= 15 ) & ( int(t[2]) < 30 ):\n",
    "    m = '15'\n",
    "elif ( int(t[2]) >= 30 ) & ( int(t[2]) < 45 ):\n",
    "    m = '30'\n",
    "elif ( int(t[2]) >= 45 ):\n",
    "    m = '45'\n",
    "\n",
    "#date1 = '20230101000000'\n",
    "date = '20' + t[0][2] + t[0][0] + t[0][1] + t[1] + m + '00'\n",
    "date = datetime.strptime(date, '%Y%m%d%H%M%S')\n",
    "\n",
    "def download_url(url, save_path, chunk_size=128):\n",
    "    r = requests.get(url, stream=True)\n",
    "    with open(save_path, 'wb') as fd:\n",
    "        for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "            fd.write(chunk)\n",
    "\n",
    "news_all= pd.DataFrame()\n",
    "download_path='C:/Users/18045/Downloads/lithium-dash-main-stephen/'\n",
    "print(\"Downloading and assembling data begin ...\")\n",
    "num_hrs=2\n",
    "lithium_preprocessing=pd.DataFrame([])\n",
    "\n",
    "while len(lithium_preprocessing)==0:\n",
    "    for i in range(4*num_hrs):    # 4* #of hrs\n",
    "\n",
    "        offset = i*15\n",
    "        l = date - timedelta(minutes=offset)\n",
    "        l = l.strftime(\"%D,%H,%M,%S\")\n",
    "        l = l.split(\",\")\n",
    "        l[0] = l[0].split(\"/\")\n",
    "\n",
    "        dateN = '20' + l[0][2] + l[0][0] + l[0][1] + l[1] + l[2] + '00'\n",
    "\n",
    "        url = 'http://data.gdeltproject.org/gdeltv2/' + dateN + '.gkg.csv.zip'\n",
    "\n",
    "        # get filename\n",
    "        name = url.split(\"/\")\n",
    "        name = name[-1]\n",
    "        names = name.split(\".\")\n",
    "\n",
    "        # download and unzip file\n",
    "        download_url(url, curdir + '/data/' + name)\n",
    "        with zipfile.ZipFile(curdir + '/data/' + name, 'r') as zip_ref:\n",
    "            zip_ref.extractall(curdir + '/data/' + names[0])\n",
    "\n",
    "        # assemble table\n",
    "        news = pd.read_csv(curdir + '/data/' + names[0] + '/' + names[0] + \".\" + names[1] + \".\" + names[2], on_bad_lines='skip', delimiter=\"\\t\", names=[\"GKGRECORDID\", \"DATE\", \"SourceCollectionIdentifier\", \"SourceCommonName\", \"DocumentIdentifier\", \"Counts\", \"V2Counts\", \"Themes\", \"V2Themes\", \"Locations\", \"V2Locations\", \"Persons\", \"V2Persons\", \"Organizations\", \"V2Organizations\", \"V2Tone\", \"Dates\", \"GCAM\", \"SharingImage\", \"RelatedImages\", \"SocialImageEmbeds\", \"SocialVideoEmbeds\", \"Quotations\", \"AllNames\", \"Amounts\", \"TranslationInfo\", \"Extras\"], dtype=\"string\", encoding = \"ISO-8859-1\")\n",
    "\n",
    "        if i==0:\n",
    "            news_15 = pd.DataFrame(news)\n",
    "        \n",
    "        news_all = pd.concat([news_all, news])\n",
    "        lithium_preprocessing = news_all[news_all['DocumentIdentifier'].str.contains('lithium')]\n",
    "        lithium_preprocessing=lithium_preprocessing.dropna(subset=['DocumentIdentifier','SourceCommonName','V2Locations','Organizations'])\n",
    "        os.remove(curdir + '/data/' + name)\n",
    "        shutil.rmtree(curdir + '/data/' + names[0])\n",
    "    num_hrs=num_hrs+1\n",
    "\n",
    "\n",
    "print(\"Downloading and assembling data finished.\")\n",
    "print(\"-----------------------------------------\")\n",
    "\n",
    "# Clean tables by matching organization names\n",
    "print(\"Begin cleaning tables by matching organization names ...\")\n",
    "\n",
    "df = pd.read_pickle('eod_dict.pickle')\n",
    "company_dict = {}\n",
    "for key,value in df.items():\n",
    "    key_temp=key.replace(',',' ')\n",
    "    company_dict[key_temp.lower()] = value.lower()\n",
    "company_key = list(company_dict.keys())\n",
    "company_key1=','.join(company_key)\n",
    "\n",
    "df_theme = pd.read_pickle('theme_sdg_mapping.pk')\n",
    "themes = list(df_theme.keys())\n",
    "\n",
    "redundant = ['co', 'plc', 'ltd', '&', 'inc', 'company', 'corp', 'corporation']\n",
    "print(\"Finished cleaning.\")\n",
    "print(\"-----------------------------------------\")\n",
    "\n",
    "def clean_bot(x):\n",
    "    \"\"\"\n",
    "    The actual robot which is gonna clean the company suffix for every company.\n",
    "    :type x: str\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    if len(x) > 0 and x[-1] in redundant:\n",
    "        del x[-1]\n",
    "        clean_bot(x)\n",
    "    return x\n",
    "\n",
    "def clean(x):\n",
    "    \"\"\"\n",
    "    Clean all the organizations from the 'Organizations' column in the raw data file.\n",
    "    The data type of every entry in the 'Organizations' column is string.\n",
    "    :type x: str\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    x = x.split(';')\n",
    "    temp = list()\n",
    "    for y in x:\n",
    "        y = y.split(' ')\n",
    "        temp.append(clean_bot(y))\n",
    "    return temp\n",
    "\n",
    "nations = set(['afghanistan', 'albania', 'algeria', 'america', 'andorra', 'angola', 'antigua', 'argentina', 'armenia',\n",
    "               'australia', 'austria', 'azerbaijan', 'bahamas', 'bahrain', 'bangladesh', 'barbados', 'belarus',\n",
    "               'belgium', 'belize', 'benin', 'bhutan', 'bissau', 'bolivia', 'bosnia', 'botswana', 'brazil', 'british',\n",
    "               'brunei', 'bulgaria', 'burkina', 'burma', 'burundi', 'cambodia', 'cameroon', 'canada', 'cape verde',\n",
    "               'central african republic', 'chad', 'chile', 'china', 'colombia', 'comoros', 'congo', 'costa rica',\n",
    "               'country debt', 'croatia', 'cuba', 'cyprus', 'czech', 'denmark', 'djibouti', 'dominica', 'east timor',\n",
    "               'ecuador', 'egypt', 'el salvador', 'emirate', 'england', 'eritrea', 'estonia', 'ethiopia', 'fiji',\n",
    "               'finland', 'france', 'gabon', 'gambia', 'georgia', 'germany', 'ghana', 'great britain', 'greece',\n",
    "               'grenada', 'grenadines', 'guatemala', 'guinea', 'guyana', 'haiti', 'herzegovina', 'honduras', 'hungary',\n",
    "               'iceland', 'in usa', 'india', 'indonesia', 'iran', 'iraq', 'ireland', 'israel', 'italy', 'ivory coast',\n",
    "               'jamaica', 'japan', 'jordan', 'kazakhstan', 'kenya', 'kiribati', 'korea', 'kosovo', 'kuwait',\n",
    "               'kyrgyzstan', 'laos', 'latvia', 'lebanon', 'lesotho', 'liberia', 'libya', 'liechtenstein', 'lithuania',\n",
    "               'luxembourg', 'macedonia', 'madagascar', 'malawi', 'malaysia', 'maldives', 'mali', 'malta', 'marshall',\n",
    "               'mauritania', 'mauritius', 'mexico', 'micronesia', 'moldova', 'monaco', 'mongolia', 'montenegro',\n",
    "               'morocco', 'mozambique', 'myanmar', 'namibia', 'nauru', 'nepal', 'netherlands', 'new zealand',\n",
    "               'nicaragua', 'niger', 'nigeria', 'norway', 'oman', 'pakistan', 'palau', 'panama', 'papua', 'paraguay',\n",
    "               'peru', 'philippines', 'poland', 'portugal', 'qatar', 'romania', 'russia', 'rwanda', 'saint kitts',\n",
    "               'samoa', 'san marino', 'santa lucia', 'sao tome', 'saudi arabia', 'scotland', 'scottish', 'senegal',\n",
    "               'serbia', 'seychelles', 'sierra leone', 'singapore', 'slovakia', 'slovenia', 'solomon', 'somalia',\n",
    "               'south africa', 'south sudan', 'spain', 'sri lanka', 'st kitts', 'st lucia', 'st. kitts', 'st. lucia',\n",
    "               'sudan', 'suriname', 'swaziland', 'sweden', 'switzerland', 'syria', 'taiwan', 'tajikistan', 'tanzania',\n",
    "               'thailand', 'tobago', 'togo', 'tonga', 'trinidad', 'tunisia', 'turkey', 'turkmenistan', 'tuvalu',\n",
    "               'uganda', 'ukraine', 'united kingdom', 'united states', 'uruguay', 'usa', 'uzbekistan', 'vanuatu',\n",
    "               'vatican', 'venezuela', 'vietnam', 'wales', 'welsh', 'yemen', 'zambia', 'zimbabwe'])\n",
    "\n",
    "def del_countries(x):\n",
    "    \"\"\"\n",
    "    Delete the strings which are names of countries in every entry from the 'Organizations' column.\n",
    "    :type: list (list of lists)\n",
    "    :rtype: list (list of lists)\n",
    "    \"\"\"\n",
    "    x = [k for k in x if ' '.join(k).lower() not in nations]\n",
    "    return x\n",
    "\n",
    "def find_index(corp_name, middle_index, company_key1):\n",
    "    \"\"\"\n",
    "    Precisely find the index of the objective string (company name) by setting ',' as delimiter and loop the string.\n",
    "    Return the index if found, return -1 if not found.\n",
    "    :type: string, int, string\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "    middle2 = company_key1[middle_index:].find(corp_name)\n",
    "    if middle2 == -1:\n",
    "        return middle2\n",
    "    elif middle2 == 0:\n",
    "        return middle2\n",
    "    elif company_key1[middle2 + middle_index - 1] == ',':\n",
    "        return middle2 + middle_index\n",
    "    else:\n",
    "        index1 = company_key1[middle2 + middle_index:].find(',')\n",
    "        if index1 == -1:\n",
    "            return -1\n",
    "        count1 = middle2 + middle_index + index1\n",
    "        return find_index(corp_name, count1, company_key1)\n",
    "\n",
    "def company_matching(x, company_key1, company_dict):\n",
    "    \"\"\"\n",
    "    Match the objective string (company name) and return a list of found companies.\n",
    "    :type: list (list of lists)\n",
    "    :type company_key1: string\n",
    "    :type company_dict: dictionary\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    x1 = [' '.join(y) for y in x]\n",
    "    childs = list()\n",
    "    for i in x1:\n",
    "        head_index = find_index(i, 0, company_key1)\n",
    "\n",
    "        if head_index == -1:\n",
    "            continue\n",
    "        else:\n",
    "            if company_key1[head_index:].find(',') == -1:\n",
    "                childs.append(company_key1[head_index:])\n",
    "            else:\n",
    "                childs.append(company_key1[head_index:head_index + company_key1[head_index:].find(',')])\n",
    "\n",
    "    if len(childs) == 0:\n",
    "        return np.nan\n",
    "\n",
    "    else:\n",
    "        return childs\n",
    "\n",
    "def return_first_v2tone_score(x):\n",
    "    \"\"\"\n",
    "    Return the first numerical value of a string in an entry of V2Tone column from raw data file.\n",
    "    :type: string\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    # l = x.split(',')\n",
    "    return float(x.split(',')[0])\n",
    "\n",
    "def clean_raw_data(data):\n",
    "    news = data[['DATE', 'DocumentIdentifier', 'V2Tone', 'Themes', 'Organizations']]          # maybe don't need to drop missing org\n",
    "    # Drop missing value for org and themes; drop duplicate value for V2Tone(there are duplicate news which can affect our research performance)\n",
    "    news_clean = news[~news['Organizations'].isna()]\n",
    "    news_clean = news_clean[~news_clean['Themes'].isna()]\n",
    "\n",
    "    # print('Cleaning corp...')\n",
    "    index = news_clean['Organizations'].apply(lambda x: clean(x))\n",
    "    news_clean['Organizations'] = index\n",
    "\n",
    "    # print('Cleaning countries...')\n",
    "    index = news_clean['Organizations'].apply(del_countries)\n",
    "    news_clean['Organizations'] = index\n",
    "\n",
    "    # print('Matching companies...')\n",
    "    index = news_clean['Organizations'].apply(lambda x: company_matching(x, company_key1, company_dict))\n",
    "    news_clean['Organizations'] = index\n",
    "    # Drop missing values\n",
    "    news_clean = news_clean.loc[index[~index.isnull()].index]\n",
    "    # Drop duplicates based on tone,themes,organization\n",
    "    news_clean['Organizations'] = news_clean['Organizations'].apply(lambda x: ','.join(x))\n",
    "    news_clean = news_clean.drop_duplicates(subset=['V2Tone', 'Themes', 'Organizations'], keep='first')\n",
    "    news_clean['Organizations'] = news_clean['Organizations'].apply(lambda x: x.split(','))\n",
    "    # Return the first value in each entry from v2tone column\n",
    "    news_clean['V2Tone'] = news_clean['V2Tone'].apply(lambda x: return_first_v2tone_score(x))\n",
    "\n",
    "    return news_clean\n",
    "\n",
    "def process_theme(news_clean):\n",
    "    final_themes = []\n",
    "    for i in range(len(news_clean['Themes'])):\n",
    "        needed_themes = []\n",
    "        row_themes = news_clean['Themes'].str.split(';').iloc[i]\n",
    "        for j in row_themes:\n",
    "            if j in themes:\n",
    "                needed_themes.append(j)\n",
    "        final_themes.append(needed_themes)\n",
    "\n",
    "    news_clean['FinalThemes'] = final_themes\n",
    "    news_clean['FinalThemes'] = [','.join(map(str, l)) for l in news_clean['FinalThemes']]\n",
    "    news_clean['FinalThemes'] = news_clean['FinalThemes'].replace(r'^\\s*$', np.nan, regex=True)\n",
    "    news_clean = news_clean[news_clean['FinalThemes'].notna()]\n",
    "\n",
    "    return news_clean\n",
    "\n",
    "\n",
    "def to_mongodb(df, collection):     # if empty df there will be problem\n",
    "    docs = []\n",
    "    if df.empty:\n",
    "        print('DataFrame is empty!')\n",
    "        return\n",
    "    for index, row in df.iterrows():\n",
    "        doc = {\n",
    "            'DATE': int(row['DATE']),\n",
    "            'DocumentIdentifier': row['DocumentIdentifier'],\n",
    "            'V2Tone': row['V2Tone'],\n",
    "            'Themes': row['Themes'],\n",
    "            'Organizations': str(row['Organizations']),\n",
    "            'FinalThemes': row['FinalThemes']\n",
    "        }\n",
    "        docs.append(doc) \n",
    "    collection.insert_many(docs)\n",
    "    print(\"Imported data to\", collection)\n",
    "\n",
    "\n",
    "def to_mongodb_map(df, collection):\n",
    "    docs = []\n",
    "    if df.empty:\n",
    "        print('DataFrame is empty!')\n",
    "        return\n",
    "    for index, row in df.iterrows():\n",
    "        doc = {\n",
    "            'DATE': int(row['DATE']),\n",
    "            'SourceCommonName': row['SourceCommonName'],\n",
    "            'DocumentIdentifier': row['DocumentIdentifier'],\n",
    "            'V2Locations': row['V2Locations'],\n",
    "            'V2Tone': row['V2Tone'],\n",
    "            'dates': row['dates'],\n",
    "        }\n",
    "        docs.append(doc) \n",
    "    collection.insert_many(docs)\n",
    "    print(\"Imported data to\", collection)\n",
    "\n",
    "# Note timestamp is PST, west coast time\n",
    "\n",
    "\n",
    "\n",
    "print(\"Begin processing last 15 minutes data ...\")\n",
    "print(\"-----------------------------------------\")\n",
    "# Final of last 15 minutes, pre data processing\n",
    "\n",
    "lithium_preprocessing_15 =lithium_preprocessing\n",
    "lithium_15 = clean_raw_data(lithium_preprocessing_15)\n",
    "\n",
    "# Fianl of last 15 minutes data\n",
    "lithium_final_15 = process_theme(lithium_15)\n",
    "\n",
    "\n",
    "to_mongodb(lithium_final_15, collection_lithium_15)\n",
    "\n",
    "\n",
    "lithium_merged_15=lithium_preprocessing_15[['DATE','DocumentIdentifier','SourceCommonName','V2Locations','V2Tone']]\n",
    "lithium_merged_15=lithium_merged_15.dropna(subset=['SourceCommonName','V2Locations'])\n",
    "lithium_merged_15['dates'] = pd.to_datetime(lithium_merged_15['DATE'], format='%Y%m%d%H%M%S')\n",
    "lithium_merged_15.sort_values(by=['DATE'],ascending=False,inplace=True)\n",
    "#lithium_merged_15.to_csv(curdir+'/lithium_merged_15.csv', index=True, header=True)\n",
    "to_mongodb_map(lithium_merged_15, collection_lithium_map_15)\n",
    "\n",
    "\n",
    "print(\"Finish processing and saving data.\")\n",
    "print(\"----------------DONE!!!!!----------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
